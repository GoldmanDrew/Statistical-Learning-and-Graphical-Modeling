{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RI2S0jiPHj7"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.stats as st\n",
        "from math import sqrt\n",
        "from IPython.display import Latex,display,Math\n",
        "print(\"Modules Imported!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2N_oGCoPHj_"
      },
      "source": [
        "# Stationary distributions of Markov chains\n",
        "In this excercise, let \n",
        "$$P=\\left[\\begin{array}{cc}\n",
        "\\frac{1}{2} & \\frac{1}{2}\\\\\n",
        "\\frac{1}{3} & \\frac{2}{3}\n",
        "\\end{array}\\right]$$\n",
        "denote the transition matrix of a Markov chain with two states $\\{W,S\\}$. Let $X_n$ denote the state of the Markov chain at time $n$. For example, $Pr(X_n=W|X_{n-1}=S)=\\frac13$. Recall that if $\\pi^{(n)}$ denotes the distribution of $X_n$, we have $\\pi^{(n+1)}=\\pi^{(n)}P$. Note that the MC is regular. So it has a unique stationary distribution $\\phi$. Observe that the stationary distribution is the left [eigenvector](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) for eigenvalue 1).\n",
        "\n",
        "_____________\n",
        "We find the eigenvalues and eigenvectors of $P$. To do this, use [`numpy.linalg`](https://docs.scipy.org/doc/numpy/reference/routines.linalg.html): \n",
        "``` python\n",
        "import numpy.linalg as la\n",
        "```\n",
        "and then use [`numpy.linalg.eig`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html#numpy.linalg.eig):\n",
        "``` python\n",
        "w,v = la.eig(P.T)\n",
        "```\n",
        "to get the eigenvaules/vectors. Find the eigenvector that corresponds to the eigenvalue 1. To do so, make sure you understand the output format of `eig`. Normalize this eigenvector such that it sums to 1 and verify that we indeed have $\\phi=\\phi P$ (use `np.dot(phi,P)`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJIs0uXMPHkB"
      },
      "outputs": [],
      "source": [
        "import numpy.linalg as la\n",
        "P=np.array([[1/2,1/2],[1/3,2/3]])\n",
        "w,v = la.eig(P.T)\n",
        "print(w)\n",
        "print(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsumDIHvPHkB"
      },
      "outputs": [],
      "source": [
        "phi = v[:,1]/sum(v[:,1])\n",
        "print(phi)\n",
        "print(np.dot(phi,P))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f-0IN0zPHkC"
      },
      "source": [
        "____________\n",
        "Pick an arbitrary initial distribution $\\pi^{(0)}$. Find $\\pi^{(n)}$ for $n=0,1,2,3,4$. Does convergence to $\\phi$ in fact occur? We also know that the rows of $P^n$ converge to the stationary distribution $\\phi$. Verify this through computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x74zvB75PHkC"
      },
      "outputs": [],
      "source": [
        "pi = [0,1]\n",
        "Pi = np.array([[1,0],[0,1]])\n",
        "P=np.array([[1/2,1/2],[1/3,2/3]])\n",
        "for i in range(5):\n",
        "    display(Math(r'P^{%i}='%i))\n",
        "    print(Pi)\n",
        "    display(Math(r'\\pi^{(%i)}='%i))\n",
        "    print(pi,\"\\n------\")\n",
        "    Pi = np.dot(Pi,P)\n",
        "    pi = np.dot(pi,P)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3eADLgOPHkD"
      },
      "source": [
        "__Sampling from Markov Chains:__ Suppose that we need to generate iid (independent and identically distributed) random samples from a given distribution $q$. Sometimes, it is difficult to do this directly, but we can construct a Markov chain whose stationary distribution is $q$. Then we can simulate this Markov chain and sample from it. Technically speaking, these samples are identically distributed but not independent. However, if there is enough of a gap between consecutive samples, they are \"approximately\" independent. For example, in the above Markov chain, if we take a sample at $n=3,6,9,\\dotsc$, we get nearly independent samples since $\\pi P^3\\simeq \\phi$ for any $\\pi$. (In practice we may decide to use all samples since dropping samples has a computational cost.)\n",
        "\n",
        "Simulate the above Markov chain by starting from an arbitrary state and take 1000 samples at steps that are multiples of 3. Verify that the distribution of these samples is the same as $\\phi$.\n",
        "\n",
        "_Hint_: Randomly transitioning to the next state where the trasition probabilities for the current state are given by $p$ is equivalent to generating a random variable that is equal to $i$ with probability $p_i$, which can be done as follows (why?):\n",
        "```python\n",
        "r = st.uniform.rvs()\n",
        "rv = np.sum(r>np.cumsum(p))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcPri1gNPHkE"
      },
      "outputs": [],
      "source": [
        "def sample_MC(P,N,skip):\n",
        "    M = len(P)\n",
        "    y = [0] * N # initializing the vector of samples\n",
        "    state = 0 # starting the chain from state 0\n",
        "    for i in range(N):\n",
        "        for j in range(skip):\n",
        "            p = P[state,:]\n",
        "            r = st.uniform.rvs()\n",
        "            state = np.sum(r>np.cumsum(p))\n",
        "        y[i] = state\n",
        "    return(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aclHoHMNPHkF"
      },
      "outputs": [],
      "source": [
        "y = sample_MC(P,1000,3)\n",
        "print('The distribution is characterized by its mean, which is 0.6')\n",
        "print('The mean of the samples from the MC is',sum(y)/len(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr1Vo8PlPHkF"
      },
      "source": [
        "<hr />\n",
        "\n",
        "### Gibbs, Metropolis, Hamiltonian Metropolis Sampling\n",
        "We will implement several MCMC sampling algorithms and compare and contrast their performance. To make things simple, the code for everything except the sampling algorithms are given. You should review and understand all the given code and make sure the code you write is compatible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yfMjBtgPHkG"
      },
      "source": [
        "<hr />\n",
        "\n",
        "__Setting up target distributions:__ First, we will define three python functions representing our target distributions, which are\n",
        "- $p_0$: a 2-d multivarate normal (__MVN__) with independent components,\n",
        "- $p_1$: a 2-d MVN with highly dependant components, and\n",
        "- $p_2$: a 2-d mixture of two MVNs.\n",
        "\n",
        "We have chosen 2-d targets to make it easier to plot the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrdVt-v6PHkG"
      },
      "outputs": [],
      "source": [
        "m = np.empty([4,2]); c = np.empty([4,2,2])\n",
        "\n",
        "m[0] = [0,0]; c[0] = [[1,0],[0,1]]\n",
        "\n",
        "m[1] = [0,0]; c[1] = 2*np.array([[1,0.98],[.98,1]])\n",
        "\n",
        "\n",
        "m[2] = [-1,-1]; c[2] = [[.7,.3],[.3,.7]]\n",
        "m[3] = [ 1, 1]; c[3] = [[.7,.3],[.3,.7]]\n",
        "\n",
        "def p0(th):\n",
        "    rv = st.multivariate_normal(mean=m[0],cov=c[0])\n",
        "    grad = np.dot(th,np.linalg.inv(c[0])) # Note how the gradient is computed\n",
        "    return [rv.pdf(th),grad]\n",
        "\n",
        "def p1(th):\n",
        "    rv = st.multivariate_normal(mean=m[1],cov=c[1])\n",
        "    grad = np.dot(th,np.linalg.inv(c[1]))\n",
        "    return [rv.pdf(th),grad]\n",
        "\n",
        "def p2(th):\n",
        "    rv1 = st.multivariate_normal(mean=m[2],cov=c[2])\n",
        "    rv2 = st.multivariate_normal(mean=m[3],cov=c[3])\n",
        "    return [rv1.pdf(th)+rv2.pdf(th), np.nan]\n",
        "\n",
        "pp = [p0,p1,p2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQPOg7ZMPHkG"
      },
      "source": [
        "<hr/>\n",
        "We will plot the mixture target distribution using contourf:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoCaC2XJPHkH"
      },
      "outputs": [],
      "source": [
        "p0([1.5,1.5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SpNOQxXPHkH"
      },
      "outputs": [],
      "source": [
        "x,y = np.mgrid[-5:5:.05,-5:5:.05]\n",
        "pos = np.empty(x.shape + (2,)) # the comma ensure (2,) is interpreted as tuple, (2) would be int\n",
        "pos[:,:,0] = x; pos[:,:,1] = y\n",
        "for p in pp:\n",
        "    plt.figure()\n",
        "    plt.contourf(x,y,p(pos)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESS7GpeDPHkH"
      },
      "source": [
        "<hr>\n",
        "\n",
        "__Sampling methods:__ Implement a Gibbs sampler, a metropolis sampler, and a Hamiltonian metropolis sampler.\n",
        "\n",
        "For the metropolis algorithm, use a Normal jump distribution. The covariance of the jump distribution is passed to the functions as an argument. The metropolis algorithm should work for all three targets.\n",
        "\n",
        "Implement the Gibbs and the HMC samplers for 2-d MVNs. So it only needs to work with $p_0$ and $p_1$ but not the mixture $p_2$. Note that the functions for $p_0$ and $p_1$ given above also pass the gradient in addition to the value of the pdf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg6UfYDiPHkI"
      },
      "outputs": [],
      "source": [
        "# metropolis with normal jumps\n",
        "def metropolis(target_p, covJump, init, N_samples):\n",
        "    # covJump: the covariance of the mustivariate normal jumping distn\n",
        "    # target_p is the target distribution from which we want to sample. this is a function\n",
        "    # N_samples is the number of samples we want\n",
        "    # th = np.empty((N,len(init))) ## th will hold the samples that we generate\n",
        "    # th[0] = init ## the starting point, i.e., the first sample\n",
        "    jump_rv = st.multivariate_normal(np.zeros(init.shape), covJump)\n",
        "    p = target_p\n",
        "    N = N_samples\n",
        "    th = np.empty((N,len(init)))\n",
        "    th[0] = init\n",
        "    \n",
        "    for i in range(1,N):\n",
        "        th_prpsl = th[i-1] + jump_rv.rvs()\n",
        "        u = st.uniform.rvs()\n",
        "        if p(th_prpsl)[0] > u * p(th[i-1])[0]:\n",
        "            th[i] = th_prpsl # accept\n",
        "        else:\n",
        "            th[i] = th[i-1]\n",
        "    return th"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRvzcKNhPHkI"
      },
      "outputs": [],
      "source": [
        "# Gibbs for 2-d normal\n",
        "def Gibbs_normal(mean, cov, init, N_samples):\n",
        "    # mean: mean of the target MVN distribution\n",
        "    # cov: covariance of the target MVN\n",
        "    # th = np.empty([N,2])\n",
        "    # th[0] = init\n",
        "    N = N_samples\n",
        "    th = np.empty([N,2])\n",
        "    th[0] = init\n",
        "    sigma = [np.sqrt(cov[0,0]),np.sqrt(cov[1,1])]\n",
        "    rho = cov[0,1]/(sigma[0]*sigma[1])\n",
        "    for i in range(1,N):\n",
        "        j = i%2\n",
        "        th[i] = th[i-1]\n",
        "        th[i,j] = st.norm.rvs(\n",
        "            loc   = mean[j]+rho*sigma[j]/sigma[1-j]*(th[i,1-j]-mean[1-j]),\n",
        "            scale = np.sqrt((1-rho**2))*sigma[j],size=1)[0]\n",
        "    return th"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQzWOBRxPHkI"
      },
      "outputs": [],
      "source": [
        "# HMC with standard normal momentum proposal, refer to McKay, \"Information Theory, Inference, and Learning Algorithms\"\n",
        "def HMC(target_p, eps, L, init, N_samples,):\n",
        "    # eps is the scale of each leapfrog step\n",
        "    # L is the number of leapfrog steps in each iteration\n",
        "    # target_p(theta) returns [pdf, gradient] for the point theta\n",
        "    # momentum_rv = ... ## proposal rv for momentum is a standard 2-d MVN\n",
        "    # th = np.empty((N,len(init)))\n",
        "    momentum_rv = st.multivariate_normal(np.zeros(len(init)), np.eye(len(init))) # proposal rv for momentum\n",
        "    p = target_p\n",
        "    N = N_samples\n",
        "    th_samples = np.empty((N,len(init)))\n",
        "    th = init\n",
        "    th_samples[0] = init\n",
        "    g = p(th)[1] # set gradient using initial th\n",
        "    E = np.log(p(th)[0]) # set objective function too\n",
        "    \n",
        "    for n in range(1,N): # N samples\n",
        "        mtm = momentum_rv.rvs() # initial momentum is Normal(0,1)\n",
        "        H = -np.dot(mtm,mtm) / 2 + E ; # evaluate H(theha,momentum)\n",
        "        th_new = th ; gnew = g ;\n",
        "        for i in range(L): # make L `leapfrog' steps\n",
        "            mtm = mtm - eps * gnew / 2 ; # make half-step in momentum\n",
        "            th_new = th_new + eps * mtm ; # make step in theta\n",
        "            gnew = p(th_new)[1] ; # find new gradient\n",
        "            mtm = mtm - eps * gnew / 2 ; # make half-step in momentum\n",
        "            \n",
        "        Enew = np.log(p(th_new)[0]); # find new value of E\n",
        "        Hnew = np.dot(mtm,mtm) / 2 + Enew ;\n",
        "        dH = Hnew - H ; # Decide whether to accept\n",
        "        if st.uniform.rvs() < np.exp(dH):\n",
        "            th = th_new; g = gnew; E = Enew\n",
        "            \n",
        "        th_samples[n] = th\n",
        "    return th_samples\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK90RCTSPHkJ"
      },
      "source": [
        "__Comparison of sampling methods:__ We now plot the samples obtained using each method for four random starting point. You need to run this 3 times to see the result for each of the targets. Note that Metropolis is the only method implemented here to handle $p_2$. Try different numbers of samples and a variety of values for the other parameters and compare the results of the methods. Describe the differences between the sampling methods based on this observations in a couple of paragraphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "QbGE8L3LPHkJ"
      },
      "outputs": [],
      "source": [
        "# target distn\n",
        "\n",
        "i = 0  # __Also try i = 1 and i = 2\n",
        "p = pp[i]\n",
        "N = 1000\n",
        "alpha = .25\n",
        "\n",
        "# the parameteres of the jumping distn\n",
        "covJump = np.array([[1,0],[0,1]])\n",
        "scale = 100\n",
        "plot = True\n",
        "# Metropolis\n",
        "f, ax = plt.subplots(2,2, sharex='col', sharey='row')\n",
        "for j in range(2):\n",
        "    for k in range(2):\n",
        "        init = st.uniform.rvs(-2,2,2)\n",
        "        th = metropolis(p, scale*covJump, init, N)\n",
        "        print(np.mean(th,0),'\\n',np.cov(th.T),'--\\n')\n",
        "        if plot:\n",
        "            plt.axes(ax[j][k])\n",
        "            plt.axis([-3,3,-3,3])\n",
        "            plt.contourf(x,y,p(pos)[0])\n",
        "            plt.plot(th[:,0],th[:,1],'k.',alpha=alpha)\n",
        "            plt.savefig('Met.pdf')\n",
        "        # with small number of samples try also ':k.' to see the order of samples\n",
        "print('==\\n')\n",
        "if i == 0 or i == 1:\n",
        "    # HMC\n",
        "    f, ax = plt.subplots(2,2)\n",
        "    for j in range(2):\n",
        "        for k in range(2):\n",
        "            init = st.uniform.rvs(-2,2,2)\n",
        "            th = HMC(p, 0.1, 10, init, N)\n",
        "            print(np.mean(th,0),'\\n',np.cov(th.T),'--\\n')\n",
        "            if plot:\n",
        "                plt.axes(ax[j][k])\n",
        "                plt.axis([-3,3,-3,3])\n",
        "                plt.contourf(x,y,p(pos)[0])\n",
        "                plt.plot(th[:,0],th[:,1],'k.',alpha=alpha)\n",
        "                plt.savefig('HMC.pdf')\n",
        "    print('==\\n')        \n",
        "    # Gibbs\n",
        "    f, ax = plt.subplots(2,2)\n",
        "    for j in range(2):\n",
        "        for k in range(2):\n",
        "            init = st.uniform.rvs(-2,2,2)\n",
        "            th = Gibbs_normal(m[i], c[i], init, N)\n",
        "            print(np.mean(th,0),'\\n',np.cov(th.T),'--\\n')\n",
        "            if plot:\n",
        "                plt.axes(ax[j][k])\n",
        "                plt.axis([-3,3,-3,3])\n",
        "                plt.contourf(x,y,p(pos)[0])\n",
        "                plt.plot(th[:,0],th[:,1],'k.',alpha=alpha) \n",
        "                plt.savefig('Gibbs.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I255vzQtPHkJ"
      },
      "outputs": [],
      "source": [
        "print(np.mean(th,0),'\\n',np.cov(th.T),'--\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGNiOnY8PHkK"
      },
      "outputs": [],
      "source": [
        "# For i = 0, if we had plotted every other sample, Gibbs would have given optimal results (no\n",
        "# dependence between the dimensions). HMC seems to perform very well too.\n",
        "# Specifically for Metropolis, it is clear that 100 samples is not enough. You can see from the\n",
        "# next set of figures that it works much better with 500 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "vFzcTX1sPHkK"
      },
      "outputs": [],
      "source": [
        "# i = 0, N = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "UXZ3IJrgPHkK"
      },
      "outputs": [],
      "source": [
        "# i = 1, N = 100\n",
        "# The situation becomes worse for Metropolis, and here is where we see the real advantage of HMC,\n",
        "# since even for N=500, Metropolis does not perform very well. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "mWe0qdYLPHkK"
      },
      "outputs": [],
      "source": [
        "# i=1, N=500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Wn5fG8u-PHkK"
      },
      "outputs": [],
      "source": [
        "# i = 2, N = 100\n",
        "# In more complex cases, Metropolis may be the only method that is feasible (the other methods are\n",
        "# impossible for this mixture but they are not as straightforward as before). In such cases, we must\n",
        "# make sure that Metropolis has converged by running parallel chains. For N=100 it is clear that \n",
        "# there is no convergence, but for N=500 the situation is better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "nkpxQGp5PHkL"
      },
      "outputs": [],
      "source": [
        "# i = 2, N = 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnWNfMrCPHkL"
      },
      "source": [
        "In conclusion, if you can implement HMC, that will likely give the best results, then Gibbs. Metropolis however is the most widely applicable methods. Also, here we did not discuss the important topic of setting the appropriate parameters for the chains. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHYzXmHoPHkL"
      },
      "source": [
        "__corner:__ The [corner package](http://corner.readthedocs.io/en/latest/) is useful for demonstrating multivariate samples. Install it (open the Anaconda Prompt and type \"pip install corner\") and then run the following code. This should show the difference between Metropolis and HMC more clearly. In particular, describe the difference between the marginals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leouRMsCPHkL"
      },
      "outputs": [],
      "source": [
        "!pip install corner\n",
        "import corner\n",
        "\n",
        "i=0\n",
        "N = 500;\n",
        "th = metropolis(pp[i], scale*covJump, init, N)\n",
        "corner.corner(th);\n",
        "\n",
        "th = HMC(pp[i], 0.1, 10, init, N)\n",
        "corner.corner(th);\n",
        "\n",
        "th = Gibbs_normal(m[i], c[i], init, N)\n",
        "corner.corner(th);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGViHp_zPHkL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "DrewGoldman_dag5wd_2021F- Lab 7 - MCMC-Sol.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}